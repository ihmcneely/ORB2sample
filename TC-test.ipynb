{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20415aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twoSampleTest as tst\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hurdat\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f616b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing...')\n",
    "BASIN         = 'EP'\n",
    "EVENT         = 'RW'\n",
    "MIN_TEST_DATE = '20130101'\n",
    "LAGTIME       = 24 # hours\n",
    "LEADTIME      = 0 # hours\n",
    "WINDTHRESH    = 35 # knots\n",
    "METAPATH = 'data/quad/'\n",
    "METASUF = '_TCdata.csv'\n",
    "RADIALPATH = 'data/quad/'\n",
    "RADIALSUF = '_rad.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326cb9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing hurdat metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/1d-simulations/hurdat.py:78: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  delimiter=',\\s+'\n"
     ]
    }
   ],
   "source": [
    "print('Importing hurdat metadata...')\n",
    "storms = hurdat.Hurdat()\n",
    "storms.storms = storms.storms.loc[storms.storms.DATE.str.slice(0, 4).astype('int') >= 1979]\n",
    "storms.identify_events(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b5d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Markov chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/1d-simulations/twoSampleTest.py:233: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  to1[ii] = self.A[2*ii+1] / (self.A[2*ii]+self.A[2*ii+1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Markov chain...\")\n",
    "chaintrain = storms.storms.loc[storms.storms.ID.str.slice(4, 8).astype('int') < int(MIN_TEST_DATE[0:4])].copy()\n",
    "chaintrain.loc[:,'Y'] = chaintrain[EVENT].astype('int')\n",
    "chaintrain = chaintrain.dropna(subset=['Y'])\n",
    "chaintrain = chaintrain.loc[chaintrain.ID.str.startswith(BASIN)]\n",
    "chain = tst.TCchain(chaintrain.Y.astype(np.int), chaintrain.ID, chaintrain.DATETIME, k=8, hurdat=storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f775227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>WIND</th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DATETIME</th>\n",
       "      <th>RI</th>\n",
       "      <th>RW</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60143</th>\n",
       "      <td>19790531</td>\n",
       "      <td>1800</td>\n",
       "      <td>TD</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-95.5</td>\n",
       "      <td>25</td>\n",
       "      <td>EP011979</td>\n",
       "      <td>ANDRES</td>\n",
       "      <td>1979-05-31 18:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60144</th>\n",
       "      <td>19790601</td>\n",
       "      <td>0000</td>\n",
       "      <td>TD</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-96.2</td>\n",
       "      <td>30</td>\n",
       "      <td>EP011979</td>\n",
       "      <td>ANDRES</td>\n",
       "      <td>1979-06-01 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60145</th>\n",
       "      <td>19790601</td>\n",
       "      <td>0600</td>\n",
       "      <td>TD</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-96.9</td>\n",
       "      <td>30</td>\n",
       "      <td>EP011979</td>\n",
       "      <td>ANDRES</td>\n",
       "      <td>1979-06-01 06:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60146</th>\n",
       "      <td>19790601</td>\n",
       "      <td>1200</td>\n",
       "      <td>TD</td>\n",
       "      <td>11.3</td>\n",
       "      <td>-97.6</td>\n",
       "      <td>30</td>\n",
       "      <td>EP011979</td>\n",
       "      <td>ANDRES</td>\n",
       "      <td>1979-06-01 12:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60147</th>\n",
       "      <td>19790601</td>\n",
       "      <td>1800</td>\n",
       "      <td>TD</td>\n",
       "      <td>11.8</td>\n",
       "      <td>-98.0</td>\n",
       "      <td>30</td>\n",
       "      <td>EP011979</td>\n",
       "      <td>ANDRES</td>\n",
       "      <td>1979-06-01 18:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76716</th>\n",
       "      <td>20121104</td>\n",
       "      <td>0600</td>\n",
       "      <td>LO</td>\n",
       "      <td>12.4</td>\n",
       "      <td>-121.3</td>\n",
       "      <td>25</td>\n",
       "      <td>EP172012</td>\n",
       "      <td>ROSA</td>\n",
       "      <td>2012-11-04 06:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76717</th>\n",
       "      <td>20121104</td>\n",
       "      <td>1200</td>\n",
       "      <td>LO</td>\n",
       "      <td>12.4</td>\n",
       "      <td>-121.3</td>\n",
       "      <td>25</td>\n",
       "      <td>EP172012</td>\n",
       "      <td>ROSA</td>\n",
       "      <td>2012-11-04 12:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76718</th>\n",
       "      <td>20121104</td>\n",
       "      <td>1800</td>\n",
       "      <td>LO</td>\n",
       "      <td>12.5</td>\n",
       "      <td>-121.4</td>\n",
       "      <td>25</td>\n",
       "      <td>EP172012</td>\n",
       "      <td>ROSA</td>\n",
       "      <td>2012-11-04 18:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76719</th>\n",
       "      <td>20121105</td>\n",
       "      <td>0000</td>\n",
       "      <td>LO</td>\n",
       "      <td>12.8</td>\n",
       "      <td>-121.6</td>\n",
       "      <td>25</td>\n",
       "      <td>EP172012</td>\n",
       "      <td>ROSA</td>\n",
       "      <td>2012-11-05 00:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76720</th>\n",
       "      <td>20121105</td>\n",
       "      <td>0600</td>\n",
       "      <td>LO</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-121.8</td>\n",
       "      <td>20</td>\n",
       "      <td>EP172012</td>\n",
       "      <td>ROSA</td>\n",
       "      <td>2012-11-05 06:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15274 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE  TIME CATEGORY   LAT    LON  WIND        ID    NAME  \\\n",
       "60143  19790531  1800       TD  11.0  -95.5    25  EP011979  ANDRES   \n",
       "60144  19790601  0000       TD  11.0  -96.2    30  EP011979  ANDRES   \n",
       "60145  19790601  0600       TD  11.0  -96.9    30  EP011979  ANDRES   \n",
       "60146  19790601  1200       TD  11.3  -97.6    30  EP011979  ANDRES   \n",
       "60147  19790601  1800       TD  11.8  -98.0    30  EP011979  ANDRES   \n",
       "...         ...   ...      ...   ...    ...   ...       ...     ...   \n",
       "76716  20121104  0600       LO  12.4 -121.3    25  EP172012    ROSA   \n",
       "76717  20121104  1200       LO  12.4 -121.3    25  EP172012    ROSA   \n",
       "76718  20121104  1800       LO  12.5 -121.4    25  EP172012    ROSA   \n",
       "76719  20121105  0000       LO  12.8 -121.6    25  EP172012    ROSA   \n",
       "76720  20121105  0600       LO  13.1 -121.8    20  EP172012    ROSA   \n",
       "\n",
       "                 DATETIME     RI     RW  Y  \n",
       "60143 1979-05-31 18:00:00  False  False  0  \n",
       "60144 1979-06-01 00:00:00  False  False  0  \n",
       "60145 1979-06-01 06:00:00  False  False  0  \n",
       "60146 1979-06-01 12:00:00  False  False  0  \n",
       "60147 1979-06-01 18:00:00  False  False  0  \n",
       "...                   ...    ...    ... ..  \n",
       "76716 2012-11-04 06:00:00  False  False  0  \n",
       "76717 2012-11-04 12:00:00  False  False  0  \n",
       "76718 2012-11-04 18:00:00  False  False  0  \n",
       "76719 2012-11-05 00:00:00  False  False  0  \n",
       "76720 2012-11-05 06:00:00  False  False  0  \n",
       "\n",
       "[15274 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chaintrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6199e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chaintrain.Y.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91cba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(METAPATH)\n",
    "ids = [file[0:8] for file in files if file.endswith(METASUF)]\n",
    "\n",
    "dfs = []\n",
    "tc_files = [(METAPATH+id+METASUF) for id in ids]\n",
    "for ii in range(len(tc_files)):\n",
    "    storm_id = ids[ii]\n",
    "    df = pd.read_csv(tc_files[ii], header=0)\n",
    "    df.rename(columns={'TIMESTAMP': 'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.round('min')\n",
    "    # interpolates latitude and wind speed between snapshots\n",
    "    # snapshots not every 30 mins -> interpolate needed\n",
    "    colstointerp = df.columns[~df.columns.isin(['timestamp', 'ID', 'NAME'])]\n",
    "    df = df.resample('0.5H', on='timestamp').mean().reset_index()\n",
    "    df[colstointerp] = df[colstointerp].interpolate()\n",
    "    # round latitude to tenths place - used to compute storm image area\n",
    "    df['LAT'] = df['LAT'].round(1)\n",
    "    df['ID'] = storm_id\n",
    "    dfs.append(df[['ID', 'timestamp', 'LAT', 'WIND']])\n",
    "\n",
    "meta = pd.concat(dfs)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab447dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading radial profiles...\n"
     ]
    }
   ],
   "source": [
    "print('Loading radial profiles...')\n",
    "dfs = []\n",
    "tc_files = [(RADIALPATH+id+RADIALSUF) for id in ids]\n",
    "for ii in range(len(ids)):\n",
    "    storm_id = ids[ii]\n",
    "    df = pd.read_csv(tc_files[ii], header=0, skiprows=[0, 2])\n",
    "    df.rename(columns={'radius': 'timestamp'}, inplace=True)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.round('min')\n",
    "    df['ID'] = storm_id\n",
    "    df.rename(columns=dict([(str(float(i)), str(i)) for i in range(5, 600 + 5, 5)]), inplace=True)\n",
    "    df.sort_values('timestamp', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs)\n",
    "data.dropna(axis=0, how='any', inplace=True)\n",
    "del dfs\n",
    "\n",
    "data = data.merge(meta, how='inner', on=['timestamp', 'ID'], suffixes=('', '_tc'))\n",
    "radcols = [str(i) for i in range(5, 400 + 5, 5)]\n",
    "data = data.loc[data['ID'].str.startswith(BASIN), :]\n",
    "\n",
    "dfs = []\n",
    "for group in storms.storms.ID.unique():\n",
    "    tmp = storms.storms.loc[storms.storms.ID == group, ['DATETIME', 'ID', EVENT]]\n",
    "    tmp = tmp.resample('0.5H', on='DATETIME').mean().reset_index()\n",
    "    tmp[EVENT] = tmp[EVENT].interpolate()\n",
    "    tmp['Y'] = np.floor(tmp[EVENT]).astype('int')\n",
    "    tmp['ID'] = group\n",
    "    tmp = tmp[['DATETIME', 'Y', 'ID']]\n",
    "    dfs.append(tmp)\n",
    "\n",
    "events = pd.concat(dfs)\n",
    "del dfs\n",
    "data = data.merge(events, how='left', left_on=['timestamp', 'ID'],\n",
    "                                      right_on=['DATETIME', 'ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05834bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing radial profile sequences...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing radial profile sequences...\")\n",
    "# slices list will be used to build full dataset\n",
    "imgs = []\n",
    "vectors = []\n",
    "total = 0\n",
    "for storm_id in data.ID.unique():\n",
    "    # sorted dataframe with just single storm\n",
    "    storm_df = data.loc[data['ID'] == storm_id, :].sort_values('timestamp')\n",
    "\n",
    "    # create numpy matrices and vectors of desired values\n",
    "    rad_mtx = storm_df[radcols].values\n",
    "\n",
    "    seq_len = LAGTIME*2 + LEADTIME*2\n",
    "    startidx = None\n",
    "    endidx = rad_mtx.shape[0] - seq_len\n",
    "    switch = False\n",
    "    storm_imgs = []\n",
    "    storm_vectors = []\n",
    "    # iterate over all 60 row images\n",
    "    for jj in range(rad_mtx.shape[0] - seq_len):\n",
    "        # Extract metadata for the \"anchor point\"\n",
    "        meta = storm_df.iloc[jj + LAGTIME].loc[['timestamp', 'ID', 'WIND', 'Y']].values\n",
    "        # Only keep between first time above threshold and last time below\n",
    "        if meta[2] >= WINDTHRESH:\n",
    "            switch = True\n",
    "            if startidx is None:\n",
    "                startidx = jj\n",
    "        if meta[2] < WINDTHRESH and switch:\n",
    "            endidx = jj\n",
    "            switch = False\n",
    "\n",
    "        total += 1 # increment on kept slice\n",
    "        # create \"slices\" of 60 rows\n",
    "        s_rad = rad_mtx[jj:(jj + seq_len), ]\n",
    "\n",
    "        storm_imgs.append(s_rad)   # element has seq_len rows\n",
    "        storm_vectors.append(meta)\n",
    "    if startidx is None:\n",
    "        continue\n",
    "    imgs = imgs + storm_imgs[startidx:endidx]\n",
    "    vectors = vectors + storm_vectors[startidx:endidx]\n",
    "x = np.stack(imgs, axis=2).transpose((2,0,1))\n",
    "del imgs\n",
    "z = pd.DataFrame(np.stack(vectors, axis=1).transpose(),\n",
    "                 columns=['timestamp', 'ID', 'WIND', 'Y'])\n",
    "del vectors\n",
    "z['idx'] = z.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057f78dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64366, 48, 80)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = np.mean(x, axis=(0,1))\n",
    "x = np.subtract(x, xmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4f5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvar = np.var(x, axis=(0,1,2))\n",
    "x = np.divide(x, np.sqrt(xvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d442162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id for id in z.ID.unique() if z.loc[z.ID==id,:].isnull().values.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fabbb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intializing test...\n"
     ]
    }
   ],
   "source": [
    "print(\"Intializing test...\")\n",
    "# z = z.iloc[[ii for ii in range(len(z.ID)) if z.ID.iloc[ii,] not in ['AL082003', 'AL142003', 'AL152000', 'AL162012']],]\n",
    "\n",
    "basinTest = tst.test2sample(train=z.loc[z['timestamp'] < MIN_TEST_DATE, :],\n",
    "                            test=z.loc[z['timestamp'] >= MIN_TEST_DATE, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b31a1ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basinTest.test_data.ID.unique().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc5bba",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8323fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadNet(nn.Module):\n",
    "    def __init__(self: int = 1000) -> None:\n",
    "        super(RadNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Try removing\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(308, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.regression(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3524d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 50, 82]              80\n",
      "              ReLU-2            [-1, 8, 50, 82]               0\n",
      "         MaxPool2d-3            [-1, 8, 25, 41]               0\n",
      "            Conv2d-4            [-1, 8, 27, 43]             584\n",
      "              ReLU-5            [-1, 8, 27, 43]               0\n",
      "         MaxPool2d-6            [-1, 8, 13, 21]               0\n",
      "            Conv2d-7            [-1, 4, 15, 23]             292\n",
      "              ReLU-8            [-1, 4, 15, 23]               0\n",
      "         MaxPool2d-9             [-1, 4, 7, 11]               0\n",
      "          Dropout-10                  [-1, 308]               0\n",
      "           Linear-11                  [-1, 256]          79,104\n",
      "             ReLU-12                  [-1, 256]               0\n",
      "           Linear-13                    [-1, 2]             514\n",
      "          Sigmoid-14                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 80,574\n",
      "Trainable params: 80,574\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.75\n",
      "Params size (MB): 0.31\n",
      "Estimated Total Size (MB): 1.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "summary(RadNet().to(device), (1, 48, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e82266",
   "metadata": {},
   "source": [
    "# Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac214c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rad, label_vec):\n",
    "        self.image = torch.unsqueeze(rad, axis=1)\n",
    "        self.y = label_vec\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.image[index,], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b4b29",
   "metadata": {},
   "source": [
    "# Define regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521507c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_regressor:\n",
    "    def __init__(self, variables=['idx'], BATCH_SIZE=512, N_EPOCHS=10000, EPOCHS_CHECK=10,\n",
    "                 LR=1e-4, LR_DECAY=0.05, PATIENCE_EARLY_STOPPING=20, test_fraction=.2):\n",
    "        # Variable here should be index in the tensor\n",
    "        self.regression = RadNet()\n",
    "        self.variables = variables\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.N_EPOCHS = N_EPOCHS\n",
    "        self.EPOCHS_CHECK = EPOCHS_CHECK\n",
    "        self.LR = LR\n",
    "        self.LR_DECAY = LR_DECAY\n",
    "        self.PATIENCE_EARLY_STOPPING = PATIENCE_EARLY_STOPPING\n",
    "        self.test_fraction = test_fraction\n",
    "        self.first_fit = True\n",
    "        self.initial_fit = None\n",
    "    \n",
    "    def train_radnet(self, Y_train, Y_test, freeze_conv=False):\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        test_prevelance = np.mean(Y_test.astype(np.float))\n",
    "        train_prevalence = np.mean(Y_train.astype(np.float))\n",
    "\n",
    "        # Draw from x by index\n",
    "        train_dataset = RadDataset(\n",
    "            rad=torch.from_numpy(x[self.train_idx,].astype(np.float64)).type(torch.Tensor),\n",
    "            label_vec=torch.from_numpy(Y_train).type(torch.LongTensor))\n",
    "        test_dataset = RadDataset(\n",
    "            rad=torch.from_numpy(x[self.test_idx,].astype(np.float64)).type(torch.Tensor),\n",
    "            label_vec=torch.from_numpy(Y_test).type(torch.LongTensor))\n",
    "\n",
    "        train_load = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=self.BATCH_SIZE, shuffle=False)\n",
    "        test_load = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = self.regression\n",
    "        \n",
    "        # freeze convolutional layers if requested\n",
    "        if freeze_conv:\n",
    "            print('Freezing convolutional layers...')\n",
    "            for child in self.regression.children():\n",
    "                if isinstance(child, nn.Conv2d):\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = False\n",
    "                if isinstance(child, nn.Linear):\n",
    "                    layer.reset_parameters()\n",
    "\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.LR)\n",
    "        avg = Y_train.mean()\n",
    "        weights = [1/(1-avg), 1/avg]\n",
    "        class_weights = torch.FloatTensor(weights).cuda()\n",
    "        loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # Creating a scheduler in case the LR decay is passed. \n",
    "        # In case the LR decay is 0, then the decay is set to happen\n",
    "        # after the last epoch (so it's equivalent to not happening)\n",
    "        step_size = self.LR_DECAY if self.LR_DECAY > 0 else self.N_EPOCHS + 1\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "\n",
    "        flnm_model = 'cnn_tmp'\n",
    "\n",
    "        training_loss, test_loss, check_loss = [], [], []\n",
    "        patience_counter = 0\n",
    "        best_median = 1e20\n",
    "        for epoch in range(self.N_EPOCHS):\n",
    "\n",
    "            if patience_counter > self.PATIENCE_EARLY_STOPPING:\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            train_loss_temp = []\n",
    "            prevs_temp = []\n",
    "            for batch_idx, (image_batch, y_batch) in enumerate(train_load):\n",
    "                image_batch, y_batch = image_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out_batch = model(image_batch)\n",
    "                loss = loss_function(out_batch, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_temp.append(loss.item())\n",
    "                prevs_temp.append(np.average(out_batch[:,1].cpu().detach().numpy()))\n",
    "\n",
    "            scheduler.step()\n",
    "            weights_average = np.ones(len(train_loss_temp))\n",
    "            weights_average[-1] = (train_dataset.image.shape[0] % self.BATCH_SIZE) / self.BATCH_SIZE\n",
    "            training_loss.append(np.average(train_loss_temp, weights=weights_average))\n",
    "            prevs = np.average(prevs_temp, weights=weights_average)\n",
    "\n",
    "            model.eval()\n",
    "            image_batch_test, y_batch_test = next(iter(test_load))\n",
    "            test_acc = y_batch_test\n",
    "            image_batch_test, y_batch_test = image_batch_test.to(device), y_batch_test.to(device)\n",
    "            out_batch_test = model(image_batch_test)\n",
    "            test_loss.append(loss_function(out_batch_test, y_batch_test).item())\n",
    "\n",
    "            if epoch == self.EPOCHS_CHECK:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'training_loss': training_loss,\n",
    "                    'test_loss': test_loss\n",
    "                }, flnm_model)\n",
    "\n",
    "            if epoch % self.EPOCHS_CHECK == 0:\n",
    "                print('Epoch: %d, Train Loss: %.4f,'\n",
    "                      ' Test Loss: %.4f, Predicted Train Fraction: %.2f,'\n",
    "                      ' True Train Fraction: %.2f,\\nPredicted Test Fraction: %.2f,' \n",
    "                      ' True Test Fraction: %.2f'% (epoch, training_loss[-1], test_loss[-1],\n",
    "                                                    prevs, train_prevalence,\n",
    "                                                    torch.mean(out_batch_test[:,1]), test_prevelance))\n",
    "\n",
    "                if epoch > (2 * self.EPOCHS_CHECK) and np.median(\n",
    "                        test_loss[-self.EPOCHS_CHECK:]) > best_median:\n",
    "                    patience_counter += 1\n",
    "                else:\n",
    "                    patience_counter = 0\n",
    "                    best_median = np.median(test_loss[-self.EPOCHS_CHECK:])\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'training_loss': training_loss,\n",
    "                        'test_loss': test_loss\n",
    "                    }, flnm_model)\n",
    "    \n",
    "    def state_load(self):       \n",
    "        checkpoint = torch.load('cnn_tmp')\n",
    "        self.regression.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, data):\n",
    "        if self.first_fit:\n",
    "            idx = data[self.variables]\n",
    "            ids = data['ID'].unique()\n",
    "            test_ids = np.random.choice(ids, size=int(ids.__len__()*self.test_fraction))\n",
    "            self.test_idx = data.loc[data.ID.isin(test_ids),'idx'].values\n",
    "            train_idx = np.array([idx for idx in data[self.variables].values if idx not in self.test_idx])\n",
    "            self.train_idx = np.squeeze(train_idx, axis=1)\n",
    "        else:\n",
    "            self.regression = copy.deepcopy(self.initial_fit)\n",
    "        \n",
    "        Y_train = [data.Y.values[ii] for ii in range(data.shape[0]) if data[self.variables].iloc[ii].values in self.train_idx]\n",
    "        Y_train = np.array(Y_train).astype(np.long)\n",
    "        Y_test = [data.Y.values[ii] for ii in range(data.shape[0]) if data[self.variables].iloc[ii].values in self.test_idx]\n",
    "        Y_test = np.array(Y_test).astype(np.long)\n",
    "        \n",
    "        self.train_radnet(Y_train, Y_test, freeze_conv=not self.first_fit)\n",
    "        self.state_load()\n",
    "        if self.first_fit:\n",
    "            self.initial_fit = copy.deepcopy(self.regression)\n",
    "            self.first_fit = False\n",
    "\n",
    "        \n",
    "    def predict(self, data):\n",
    "        new_dataset = RadDataset(\n",
    "            rad=torch.from_numpy(x[data.idx,].astype(np.float64)).type(torch.Tensor),\n",
    "            label_vec=torch.from_numpy(data.Y.values.astype(int)).type(torch.LongTensor))\n",
    "        \n",
    "        new_loader = torch.utils.data.DataLoader(dataset=new_dataset, batch_size=data.shape[0], shuffle=False)\n",
    "        x_new, y_new = next(iter(new_loader))\n",
    "        y_pred = self.regression(x_new.to(device)).cpu().detach().numpy()\n",
    "        y_pred = np.divide(y_pred, np.expand_dims(np.sum(y_pred, axis=1), axis=1))\n",
    "        \n",
    "        return y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = cnn_regressor(EPOCHS_CHECK=20, PATIENCE_EARLY_STOPPING=10, \n",
    "                    LR=1e-5, LR_DECAY=0.01, test_fraction=.4)\n",
    "reg.fit(basinTest.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9153a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(basinTest.test_data)\n",
    "y_valid = basinTest.test_data.Y.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = RadDataset(\n",
    "            rad=torch.from_numpy(x[basinTest.test_data.idx,].astype(np.float64)).type(torch.Tensor),\n",
    "            label_vec=torch.from_numpy(basinTest.test_data.Y.values.astype(int)).type(torch.LongTensor))\n",
    "valid_load = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=17969, shuffle=False)\n",
    "x_valid, y_valid = next(iter(valid_load))\n",
    "y_valid = y_valid.detach().numpy()\n",
    "y_pred = reg.predict(basinTest.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstar = 0.5\n",
    "tp = sum([(y_pred[ii] > pstar) for ii in range(y_valid.shape[0]) if y_valid[ii] == 1])\n",
    "fp = sum([(y_pred[ii] > pstar) for ii in range(y_valid.shape[0]) if y_valid[ii] != 1])\n",
    "tn = sum([(y_pred[ii] < pstar) for ii in range(y_valid.shape[0]) if y_valid[ii] != 1])\n",
    "fn = sum([(y_pred[ii] < pstar) for ii in range(y_valid.shape[0]) if y_valid[ii] == 1])\n",
    "tpr = tp/(tp+fn)\n",
    "tnr = tn/(tn+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77663b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, tnr, (tpr+tnr)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51830f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testreg = cnn_regressor(N_EPOCHS=1000, EPOCHS_CHECK=20, PATIENCE_EARLY_STOPPING=10, \n",
    "                        LR=2e-5, LR_DECAY=0.01, test_fraction=.4)\n",
    "basinTest.test(chain, testreg, pb=True, groupvar='ID', B=100)\n",
    "\n",
    "basinTest.test_data.to_csv('results/ENP-RW-TestData-update.csv')\n",
    "basinTest.train_data.to_csv('results/ENP-RW-TrainData-update.csv')\n",
    "np.savetxt('results/ENP-RW-NullDsn-functional-update.csv', basinTest.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basinTest.get_global()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
